# -*- coding: utf-8 -*-
"""Homework7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1acA58xuroAXhxR3x7u2RdyM9ZSlJA5O5
"""

# Importing necessary modules
from airflow import DAG
from airflow.decorators import task
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
import snowflake.connector
from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator
from datetime import datetime, timedelta

# Establishing connection to Snowflake
def return_snowflake_conn():
    hook = SnowflakeHook(snowflake_conn_id='snowflake_conn') # Initialize the SnowflakeHook
    conn = hook.get_conn()
    return conn.cursor() # Created a cursor object to work with databases

# Creating Tables
create_user_session_channel_table = """
CREATE TABLE IF NOT EXISTS dev.raw_data.user_session_channel (
    userId int not NULL,
    sessionId varchar(32) primary key,
    channel varchar(32) default 'direct'
);
"""

create_session_timestamp_table = """
CREATE TABLE IF NOT EXISTS dev.raw_data.session_timestamp (
    sessionId varchar(32) primary key,
    ts timestamp
);
"""

# Creating Stage
create_stage = """
CREATE OR REPLACE STAGE dev.raw_data.blob_stage
url = 's3://s3-geospatial/readonly/'
file_format = (type = csv, skip_header = 1, field_optionally_enclosed_by = '"');
"""

# Copy Into Commands to Move Data from S3
copy_cmd_user_session_channel = """
COPY INTO dev.raw_data.user_session_channel
FROM @dev.raw_data.blob_stage/user_session_channel.csv;
"""

copy_cmd_session_timestamp = """
COPY INTO dev.raw_data.session_timestamp
FROM @dev.raw_data.blob_stage/session_timestamp.csv;
"""

# Define the DAG
with DAG(
    'SessionToSnowflake',
    default_args={
        "owner": "Shatayu Thakur",
        "email": ["shatayu.thakur@sjsu.edu"],
        "email_on_failure": True,
        "email_on_retry": True,
        "email_on_success": True,
        "retries": 1,
        "retry_delay": timedelta(minutes=5),
    },
    description='This DAG Populates Snowflake Tables from S3',
    schedule_interval='0 * * * *',  # Trigger every hour
    start_date=datetime(2024, 10, 21),
    catchup=False,
    tags=["ETL"],
) as dag:

    # Creating user_session_channel table
    create_user_session_channel = SnowflakeOperator(
        task_id='create_user_session_channel_table',
        sql=create_user_session_channel_table,
        snowflake_conn_id='snowflake_conn',
        database='dev',
        schema='raw_data'
    )

    # Creating session_timestamp table
    create_session_timestamp = SnowflakeOperator(
        task_id='create_session_timestamp_table',
        sql=create_session_timestamp_table,
        snowflake_conn_id='snowflake_conn',
        database='dev',
        schema='raw_data'
    )

    # Creating the stage in Snowflake
    create_s3_stage = SnowflakeOperator(
        task_id='create_s3_stage',
        sql=create_stage,
        snowflake_conn_id='snowflake_conn',
        database='dev',
        schema='raw_data'
    )

    # Copying data into user_session_channel table
    load_user_session_channel = SnowflakeOperator(
        task_id='load_user_session_channel',
        sql=copy_cmd_user_session_channel,
        snowflake_conn_id='snowflake_conn',
        database='dev',
        schema='raw_data'
    )

    # Copying data into session_timestamp table
    load_session_timestamp = SnowflakeOperator(
        task_id='load_session_timestamp',
        sql=copy_cmd_session_timestamp,
        snowflake_conn_id='snowflake_conn',
        database='dev',
        schema='raw_data'
    )

    # Set the task dependencies
    [create_user_session_channel, create_session_timestamp] >> create_s3_stage >> [load_user_session_channel, load_session_timestamp]